##testing
trial_200 <- list(vendorlist[2:5, 3])
df <- data.frame(trial_200 = trial_200)
###Create a 9xnrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
for (i in 1:3) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
trial_200 <- list(vendorlist[2:nrow(vendorlist) ,3 ])
##testing
trial_200 <- list(vendorlist[2:5, 3])
df <- data.frame(trial_200 = trial_200)
###Create a 9xnrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
for (i in 1:3) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "") %>%
as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
#Scraping function for Number of search results and phone & addresses
for (i in 1:3) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "") %>%
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
#Scraping function for Number of search results and phone & addresses
for (i in 1:3) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "") %>%
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#Scraping function for Number of search results and phone & addresses
for (i in 1:3) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
as.integer(276,99)
as.integer(276,990)
as.numeric(276,990)
str(table)
summary(table)
trial_200 <- list(vendorlist[2:nrow(vendorlist) ,3 ])
##testing
trial_200 <- list(vendorlist[2:5, 3])
df <- data.frame(trial_200 = trial_200)
###Create a 9xnrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
for (i in 1:4) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
###Create a 9xnrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
for (i in 1:4) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
View(table)
vendorlist <- read.csv("VendorNamesToScrape_v2_27-04-2018.csv", sep = "\t", quote = "", header = F)
##testing
trial_200 <- list(vendorlist[2:6, 3])
df <- data.frame(trial_200 = trial_200)
###Create a 9xnrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
for (i in 1:4) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
#Scraping function for Number of search results and phone & addresses
for (i in 1:5) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
#Scraping function for Number of search results and phone & addresses
for (i in 3:5) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
#  table[i, 8] <- address %>% str_replace("Address:", "")
# table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
#Scraping function for Number of search results and phone & addresses
for (i in 3:10) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
#  table[i, 8] <- address %>% str_replace("Address:", "")
# table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
trial_200 <- (vendorlist[2:nrow(vendorlist) ,3 ])
#import list/file of vendor names
vendorlist <- read.csv("VendorNamesToScrape_v2_27-04-2018.csv", sep = "\t", quote = "", header = F)
trial_200 <- (vendorlist[1:27 ,3 ])
#Scraping function for Number of search results and phone & addresses
for (i in 3:15) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
#  table[i, 8] <- address %>% str_replace("Address:", "")
# table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
length(df)
length(table[,1])
vendorlist <- read.csv("VendorNamesToScrape_v2_27-04-2018.csv", sep = "\t", quote = "", header = F)
trial_200 <- list(vendorlist[2:nrow(vendorlist) ,3 ])
df <- data.frame(trial_200 = trial_200)
#first col - vendornames
table[,1] <- as.character(df[,1])
#import list/file of vendornames
vendorlist <- read.csv("VendorNamesToScrape_v2_27-04-2018.csv", sep = "\t", quote = "", header = F)
#import list/file of vendornames
vendorlist <- read.csv("VendorNamesToScrape_v2_27-04-2018.csv", sep = "\t", quote = "", header = F)
trial_200 <- list(vendorlist[1:nrow(vendorlist) ,3 ])
df <- data.frame(trial_200 = trial_200)
###Create a 9xnrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
for (i in 3:15) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
}
#import list/file of vendornames
vendorlist <- read.csv("##vendorlist_filename.csv", sep = "\t", quote = "", header = F)
trial_200 <- list(vendorlist[1:nrow(vendorlist) ,3 ])
df <- data.frame(trial_200 = trial_200)
###Create a  9 x nrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
batch <- 5
for (i in 1:batch) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape top 5 URLs
Sys.sleep(5)
xxx <-read_html(url) %>%
html_nodes(css= ".s div") %>%
html_text()
Sys.sleep(7)
#return the URLs only
pattern <- c(".com|.gov|.au" )
xxx <- array(grep(pattern, xxx, value = TRUE))
xxx <- xxx[1:5]
for(g in 1:nrow(xxx)){
table[i, g+2] <- xxx[g]
}
}
vendorlist <- read.csv("VendorNamesToScrape_v2_27-04-2018.csv", sep = "\t", quote = "", header = F)
#import list/file of vendornames
vendorlist <- read.csv("##vendorlist_filename.csv", sep = "\t", quote = "", header = F)
trial_200 <- list(vendorlist[1:nrow(vendorlist) ,3 ])
vendorlist <- read.csv("VendorNamesToScrape_v2_27-04-2018.csv", sep = "\t", quote = "", header = F)
trial_200 <- list(vendorlist[1:nrow(vendorlist) ,3 ])
df <- data.frame(trial_200 = trial_200)
###Create a  9 x nrow matrix to store scraped information
table <- matrix(ncol = 9, nrow = nrow(df))
#first col - vendornames
table[,1] <- as.character(df[,1])
#Scraping function for Number of search results and phone & addresses
batch <- 5
for (i in 1:batch) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape top 5 URLs
Sys.sleep(5)
xxx <-read_html(url) %>%
html_nodes(css= ".s div") %>%
html_text()
Sys.sleep(7)
#return the URLs only
pattern <- c(".com|.gov|.au" )
xxx <- array(grep(pattern, xxx, value = TRUE))
xxx <- xxx[1:5]
for(g in 1:nrow(xxx)){
table[i, g+2] <- xxx[g]
}
}
View(table)
#Scraping function for Number of search results and phone & addresses
batch <- 5
for (i in 2:batch) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape number of search results
table[i,2] <- read_html(url) %>%
html_nodes(css= "#resultStats") %>%
html_text() %>%
str_replace("About", "") %>%
str_replace("results", "")
#as.integer()
#System sleep wil suspend execution for a time given in secs
Sys.sleep(8)
#scrape phone & address
yyy <- read_html(url) %>%
html_nodes(css= "#rhs_block div") %>%
html_text()
address <- as.character(grep("Address", yyy, value = TRUE)[4])
phoneno <- as.character(grep("Phone", yyy, value = TRUE)[4])
table[i, 8] <- address %>% str_replace("Address:", "")
table[i, 9] <- phoneno %>% str_replace("Phone: ", "")
Sys.sleep(8)
}
xxx
#Scraping function for Number of search results and phone & addresses
for (i in 1:batch) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape top 5 URLs
Sys.sleep(7)
xxx <-read_html(url) %>%
html_nodes(css= ".s div") %>%
html_text()
#return the URLs only
pattern <- c(".com|.gov|.au" )
xxx <- array(grep(pattern, xxx, value = TRUE))
xxx <- xxx[1:5]
for(g in 1:nrow(xxx)){
table[i, g+2] <- xxx[g] %>% str_replace(c("Cached", "/Similar", "/CachedSimlar", "www.", "https://"), "")
}
}
?str_replace
?str_remove_all
#Scraping function for Number of search results and phone & addresses
for (i in 1:batch) {
print(paste0("finding the url for:", table[i,1]))
#create URL by combining URLhead and vendor name
url <- paste0("https://www.google.com/search?q=",url_encode(table[i,1]))
#scrape top 5 URLs
Sys.sleep(7)
xxx <-read_html(url) %>%
html_nodes(css= ".s div") %>%
html_text()
#return the URLs only
pattern <- c(".com|.gov|.au" )
xxx <- array(grep(pattern, xxx, value = TRUE))
xxx <- xxx[1:5]
for(g in 1:nrow(xxx)){
table[i, g+2] <- xxx[g] %>% str_remove_all(c("Cached", "/Similar", "/CachedSimlar", "www.", "https://"), "")
}
}
?str_remove_all
#Scraping function for Number of search results and phone & addresses
string <- c("Cached", "/Similar", "/CachedSimlar", "www.", "https://")
str_remove_all(string, "")
str_remove_all(string, " ")
xxx
xxx %>% str_remove_all(string, "")
xxx %>% str_remove_all(string, "")
xxx %>% str_remove_all(string)
string
xxx %>% str_replace(string, "")
xxx %>% str_replace_all(string, "")
setwd("~/Data Scrapping/Packaging")
run_exe("abn_lookup.exe")
str_replace_all(lkas_1, "_", "w")
str_replace_all("lkas_1", "_", "w")
str_replace_all(xxx, string, "")
str_replace_all(xxx, Cached, "")
str_replace_all(xxx, "Cached", "")
str_replace_all(xxx, c("Cached", "https://", "www."), "")
str_replace_all(xxx, ("https://", "www."), "")
str_replace_all(xxx, ("https://"), "")
